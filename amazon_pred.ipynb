{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "796be546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e708b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (43739, 16)\n",
      "columns: ['Order_ID', 'Agent_Age', 'Agent_Rating', 'Store_Latitude', 'Store_Longitude', 'Drop_Latitude', 'Drop_Longitude', 'Order_Date', 'Order_Time', 'Pickup_Time', 'Weather', 'Traffic', 'Vehicle', 'Area', 'Delivery_Time', 'Category']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Agent_Age</th>\n",
       "      <th>Agent_Rating</th>\n",
       "      <th>Store_Latitude</th>\n",
       "      <th>Store_Longitude</th>\n",
       "      <th>Drop_Latitude</th>\n",
       "      <th>Drop_Longitude</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Order_Time</th>\n",
       "      <th>Pickup_Time</th>\n",
       "      <th>Weather</th>\n",
       "      <th>Traffic</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Area</th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ialx566343618</td>\n",
       "      <td>37</td>\n",
       "      <td>4.9</td>\n",
       "      <td>22.745049</td>\n",
       "      <td>75.892471</td>\n",
       "      <td>22.765049</td>\n",
       "      <td>75.912471</td>\n",
       "      <td>2022-03-19</td>\n",
       "      <td>11:30:00</td>\n",
       "      <td>11:45:00</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>High</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>Urban</td>\n",
       "      <td>120</td>\n",
       "      <td>Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akqg208421122</td>\n",
       "      <td>34</td>\n",
       "      <td>4.5</td>\n",
       "      <td>12.913041</td>\n",
       "      <td>77.683237</td>\n",
       "      <td>13.043041</td>\n",
       "      <td>77.813237</td>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>19:45:00</td>\n",
       "      <td>19:50:00</td>\n",
       "      <td>Stormy</td>\n",
       "      <td>Jam</td>\n",
       "      <td>scooter</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>165</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>njpu434582536</td>\n",
       "      <td>23</td>\n",
       "      <td>4.4</td>\n",
       "      <td>12.914264</td>\n",
       "      <td>77.678400</td>\n",
       "      <td>12.924264</td>\n",
       "      <td>77.688400</td>\n",
       "      <td>2022-03-19</td>\n",
       "      <td>08:30:00</td>\n",
       "      <td>08:45:00</td>\n",
       "      <td>Sandstorms</td>\n",
       "      <td>Low</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>Urban</td>\n",
       "      <td>130</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rjto796129700</td>\n",
       "      <td>38</td>\n",
       "      <td>4.7</td>\n",
       "      <td>11.003669</td>\n",
       "      <td>76.976494</td>\n",
       "      <td>11.053669</td>\n",
       "      <td>77.026494</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>18:10:00</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>Medium</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>105</td>\n",
       "      <td>Cosmetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zguw716275638</td>\n",
       "      <td>32</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.972793</td>\n",
       "      <td>80.249982</td>\n",
       "      <td>13.012793</td>\n",
       "      <td>80.289982</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>13:45:00</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>High</td>\n",
       "      <td>scooter</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>150</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fxuu788413734</td>\n",
       "      <td>22</td>\n",
       "      <td>4.8</td>\n",
       "      <td>17.431668</td>\n",
       "      <td>78.408321</td>\n",
       "      <td>17.461668</td>\n",
       "      <td>78.438321</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>21:20:00</td>\n",
       "      <td>21:30:00</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Jam</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>Urban</td>\n",
       "      <td>130</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>njmo150975311</td>\n",
       "      <td>33</td>\n",
       "      <td>4.7</td>\n",
       "      <td>23.369746</td>\n",
       "      <td>85.339820</td>\n",
       "      <td>23.479746</td>\n",
       "      <td>85.449820</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>19:15:00</td>\n",
       "      <td>19:30:00</td>\n",
       "      <td>Fog</td>\n",
       "      <td>Jam</td>\n",
       "      <td>scooter</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>200</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jvjc772545076</td>\n",
       "      <td>35</td>\n",
       "      <td>4.6</td>\n",
       "      <td>12.352058</td>\n",
       "      <td>76.606650</td>\n",
       "      <td>12.482058</td>\n",
       "      <td>76.736650</td>\n",
       "      <td>2022-03-14</td>\n",
       "      <td>17:25:00</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Medium</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>Metropolitian</td>\n",
       "      <td>160</td>\n",
       "      <td>Snacks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Order_ID  Agent_Age  Agent_Rating  Store_Latitude  Store_Longitude  \\\n",
       "0  ialx566343618         37           4.9       22.745049        75.892471   \n",
       "1  akqg208421122         34           4.5       12.913041        77.683237   \n",
       "2  njpu434582536         23           4.4       12.914264        77.678400   \n",
       "3  rjto796129700         38           4.7       11.003669        76.976494   \n",
       "4  zguw716275638         32           4.6       12.972793        80.249982   \n",
       "5  fxuu788413734         22           4.8       17.431668        78.408321   \n",
       "6  njmo150975311         33           4.7       23.369746        85.339820   \n",
       "7  jvjc772545076         35           4.6       12.352058        76.606650   \n",
       "\n",
       "   Drop_Latitude  Drop_Longitude  Order_Date Order_Time Pickup_Time  \\\n",
       "0      22.765049       75.912471  2022-03-19   11:30:00    11:45:00   \n",
       "1      13.043041       77.813237  2022-03-25   19:45:00    19:50:00   \n",
       "2      12.924264       77.688400  2022-03-19   08:30:00    08:45:00   \n",
       "3      11.053669       77.026494  2022-04-05   18:00:00    18:10:00   \n",
       "4      13.012793       80.289982  2022-03-26   13:30:00    13:45:00   \n",
       "5      17.461668       78.438321  2022-03-11   21:20:00    21:30:00   \n",
       "6      23.479746       85.449820  2022-03-04   19:15:00    19:30:00   \n",
       "7      12.482058       76.736650  2022-03-14   17:25:00    17:30:00   \n",
       "\n",
       "      Weather  Traffic      Vehicle            Area  Delivery_Time  \\\n",
       "0       Sunny    High   motorcycle           Urban             120   \n",
       "1      Stormy     Jam      scooter   Metropolitian             165   \n",
       "2  Sandstorms     Low   motorcycle           Urban             130   \n",
       "3       Sunny  Medium   motorcycle   Metropolitian             105   \n",
       "4      Cloudy    High      scooter   Metropolitian             150   \n",
       "5      Cloudy     Jam   motorcycle           Urban             130   \n",
       "6         Fog     Jam      scooter   Metropolitian             200   \n",
       "7      Cloudy  Medium   motorcycle   Metropolitian             160   \n",
       "\n",
       "      Category  \n",
       "0     Clothing  \n",
       "1  Electronics  \n",
       "2       Sports  \n",
       "3    Cosmetics  \n",
       "4         Toys  \n",
       "5         Toys  \n",
       "6         Toys  \n",
       "7       Snacks  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      " Weather            91\n",
      "Agent_Rating       54\n",
      "Agent_Age           0\n",
      "Order_ID            0\n",
      "Store_Longitude     0\n",
      "Drop_Latitude       0\n",
      "Drop_Longitude      0\n",
      "Store_Latitude      0\n",
      "Order_Date          0\n",
      "Order_Time          0\n",
      "Pickup_Time         0\n",
      "Traffic             0\n",
      "Vehicle             0\n",
      "Area                0\n",
      "Delivery_Time       0\n",
      "Category            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1-load_inspect.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# path you provided\n",
    "path = r\"D:\\Amazon_Delivery_Time_Prediction\\amazon_delivery.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "print(\"shape:\", df.shape)\n",
    "print(\"columns:\", df.columns.tolist())\n",
    "display(df.head(8))   # in a notebook; in a script use print(df.head())\n",
    "print(\"\\nMissing values per column:\\n\", df.isnull().sum().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e6b1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-clean_feature_engineer.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load\n",
    "df = pd.read_csv(r\"D:\\Amazon_Delivery_Time_Prediction\\amazon_delivery.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "536c5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) strip whitespace for string columns to remove trailing spaces like 'High '\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc000955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Delivery_Time to numeric (looks like minutes)\n",
    "df['Delivery_Time'] = pd.to_numeric(df['Delivery_Time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b297b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse datetimes\n",
    "df['order_datetime'] = pd.to_datetime(df['Order_Date'].astype(str) + ' ' + df['Order_Time'].astype(str), errors='coerce')\n",
    "df['pickup_datetime'] = pd.to_datetime(df['Order_Date'].astype(str) + ' ' + df['Pickup_Time'].astype(str), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75ea1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If pickup looks earlier than order (possible next-day pickup), add 1 day\n",
    "mask = (df['pickup_datetime'] < df['order_datetime']) & df['pickup_datetime'].notnull() & df['order_datetime'].notnull()\n",
    "df.loc[mask, 'pickup_datetime'] += pd.Timedelta(days=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c746b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime if not already\n",
    "df['order_datetime'] = pd.to_datetime(df['order_datetime'])\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "# Apply the day adjustment for rows where pickup < order\n",
    "mask = df['pickup_datetime'] < df['order_datetime']\n",
    "df.loc[mask, 'pickup_datetime'] += pd.Timedelta(days=1)\n",
    "\n",
    "# Now calculate pickup delay in minutes\n",
    "df['pickup_delay_mins'] = (df['pickup_datetime'] - df['order_datetime']).dt.total_seconds() / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c58fa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine distance (km) between store and drop coordinates\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # vectorized implementation using numpy\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    R = 6371  # Earth radius in km\n",
    "    return R * c\n",
    "\n",
    "df['distance_km'] = haversine_km(df['Store_Latitude'], df['Store_Longitude'],\n",
    "                                 df['Drop_Latitude'], df['Drop_Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea91d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time features\n",
    "df['order_hour'] = df['order_datetime'].dt.hour\n",
    "df['order_dayofweek'] = df['order_datetime'].dt.dayofweek\n",
    "\n",
    "# impute Agent_Rating with median (minority missing)\n",
    "df['Agent_Rating'] = pd.to_numeric(df['Agent_Rating'], errors='coerce')\n",
    "df['Agent_Rating'].fillna(df['Agent_Rating'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "339ca9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop small number of rows missing key time info (you can impute instead if needed)\n",
    "df = df.dropna(subset=['order_datetime', 'Order_Time', 'Traffic', 'Weather'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe69b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning, shape: (43648, 22)\n",
      "                     count        mean         std        min        25%  \\\n",
      "Delivery_Time      43648.0  124.914475   51.933163  10.000000  90.000000   \n",
      "distance_km        43648.0   27.255432  303.815765   1.465067   4.663432   \n",
      "pickup_delay_mins  43648.0    9.991294    4.086680   5.000000   5.000000   \n",
      "order_hour         43648.0   17.425976    4.818494   0.000000  15.000000   \n",
      "\n",
      "                          50%         75%          max  \n",
      "Delivery_Time      125.000000  160.000000   270.000000  \n",
      "distance_km          9.220419   13.682165  6884.726399  \n",
      "pickup_delay_mins   10.000000   15.000000    15.000000  \n",
      "order_hour          19.000000   21.000000    23.000000  \n"
     ]
    }
   ],
   "source": [
    "# final quick check\n",
    "print(\"After cleaning, shape:\", df.shape)\n",
    "print(df[['Delivery_Time', 'distance_km', 'pickup_delay_mins', 'order_hour']].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5b18401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (43648, 22)\n",
      "After dropping NaNs: (43648, 22)\n",
      "Dropped 0 rows\n",
      "\n",
      "Target variable (Delivery_Time) stats:\n",
      "count    43648.000000\n",
      "mean       124.914475\n",
      "std         51.933163\n",
      "min         10.000000\n",
      "25%         90.000000\n",
      "50%        125.000000\n",
      "75%        160.000000\n",
      "max        270.000000\n",
      "Name: Delivery_Time, dtype: float64\n",
      "\n",
      "Train set size: 34918 (80%)\n",
      "Test set size: 8730 (20%)\n",
      "\n",
      "Fitting preprocessor on training data...\n",
      "Transformed training shape: (34918, 39)\n",
      "Transformed test shape: (8730, 39)\n",
      "\n",
      "Total features after encoding: 39\n",
      "- Numeric features: 6\n",
      "- Categorical features (after OHE): 33\n",
      "\n",
      "Preprocessor saved to 'preprocessor.pkl'\n",
      "Train/test data saved to .npy files\n",
      "\n",
      "âœ… Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# 3-prepare_train.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# Define features\n",
    "num_feats = ['distance_km', 'Agent_Age', 'Agent_Rating', 'pickup_delay_mins', 'order_hour', 'order_dayofweek']\n",
    "cat_feats = ['Weather', 'Traffic', 'Vehicle', 'Area', 'Category']\n",
    "\n",
    "# Drop any leftover NaNs in selected features\n",
    "print(\"Original shape:\", df.shape)\n",
    "df_model = df.dropna(subset=num_feats + cat_feats + ['Delivery_Time']).copy()\n",
    "print(f\"After dropping NaNs: {df_model.shape}\")\n",
    "print(f\"Dropped {df.shape[0] - df_model.shape[0]} rows\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_model[num_feats + cat_feats]\n",
    "y = df_model['Delivery_Time']  # minutes\n",
    "\n",
    "print(f\"\\nTarget variable (Delivery_Time) stats:\")\n",
    "print(y.describe())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.20, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {X_train.shape[0]} ({(1-0.20)*100:.0f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} ({0.20*100:.0f}%)\")\n",
    "\n",
    "# Preprocessor: scale numeric, OHE categorical\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_feats),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_feats),\n",
    "    ], \n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on training data\n",
    "print(\"\\nFitting preprocessor on training data...\")\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Transformed training shape: {X_train_transformed.shape}\")\n",
    "print(f\"Transformed test shape: {X_test_transformed.shape}\")\n",
    "\n",
    "# Get feature names after transformation\n",
    "try:\n",
    "    feature_names = (\n",
    "        num_feats + \n",
    "        preprocessor.named_transformers_['cat'].get_feature_names_out(cat_feats).tolist()\n",
    "    )\n",
    "    print(f\"\\nTotal features after encoding: {len(feature_names)}\")\n",
    "    print(f\"- Numeric features: {len(num_feats)}\")\n",
    "    print(f\"- Categorical features (after OHE): {len(feature_names) - len(num_feats)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get feature names: {e}\")\n",
    "\n",
    "# Optional: Save preprocessor for later use\n",
    "try:\n",
    "    with open('preprocessor.pkl', 'wb') as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    print(\"\\nPreprocessor saved to 'preprocessor.pkl'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save preprocessor: {e}\")\n",
    "\n",
    "# Optional: Save train/test splits\n",
    "try:\n",
    "    np.save('X_train.npy', X_train_transformed)\n",
    "    np.save('X_test.npy', X_test_transformed)\n",
    "    np.save('y_train.npy', y_train.values)\n",
    "    np.save('y_test.npy', y_test.values)\n",
    "    print(\"Train/test data saved to .npy files\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save data: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb92a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Train shape: (34918, 39)\n",
      "Test shape: (8730, 39)\n"
     ]
    }
   ],
   "source": [
    "# 4-train_and_eval.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "X_train_transformed = np.load('X_train.npy')\n",
    "X_test_transformed = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "print(f\"Train shape: {X_train_transformed.shape}\")\n",
    "print(f\"Test shape: {X_test_transformed.shape}\")\n",
    "\n",
    "def evaluate(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model and print metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)  # Calculate RMSE manually\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  MAE:  {mae:.2f} mins\")\n",
    "    print(f\"  RMSE: {rmse:.2f} mins\")\n",
    "    print(f\"  RÂ²:   {r2:.4f}\")\n",
    "    \n",
    "    return mae, rmse, r2, y_pred\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a1f00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Training Linear Regression...\n",
      "\n",
      "LinearRegression:\n",
      "  MAE:  26.40 mins\n",
      "  RMSE: 33.48 mins\n",
      "  RÂ²:   0.5873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n[1/3] Training Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_transformed, y_train)\n",
    "lr_mae, lr_rmse, lr_r2, lr_pred = evaluate(lr_model, X_test_transformed, y_test, \"LinearRegression\")\n",
    "results['Linear Regression'] = {'MAE': lr_mae, 'RMSE': lr_rmse, 'RÂ²': lr_r2, 'predictions': lr_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d181f69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/3] Training Random Forest...\n",
      "\n",
      "RandomForest:\n",
      "  MAE:  17.04 mins\n",
      "  RMSE: 22.14 mins\n",
      "  RÂ²:   0.8194\n"
     ]
    }
   ],
   "source": [
    "# 2. Random Forest\n",
    "print(\"\\n[2/3] Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=15, \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_model.fit(X_train_transformed, y_train)\n",
    "rf_mae, rf_rmse, rf_r2, rf_pred = evaluate(rf_model, X_test_transformed, y_test, \"RandomForest\")\n",
    "results['Random Forest'] = {'MAE': rf_mae, 'RMSE': rf_rmse, 'RÂ²': rf_r2, 'predictions': rf_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "284af604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/3] Training Gradient Boosting...\n",
      "\n",
      "GradientBoosting:\n",
      "  MAE:  17.38 mins\n",
      "  RMSE: 22.35 mins\n",
      "  RÂ²:   0.8160\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Gradient Boosting\n",
    "print(\"\\n[3/3] Training Gradient Boosting...\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=6, \n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "gb_model.fit(X_train_transformed, y_train)\n",
    "gb_mae, gb_rmse, gb_r2, gb_pred = evaluate(gb_model, X_test_transformed, y_test, \"GradientBoosting\")\n",
    "results['Gradient Boosting'] = {'MAE': gb_mae, 'RMSE': gb_rmse, 'RÂ²': gb_r2, 'predictions': gb_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7963565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "\n",
      "             Model  MAE (mins)  RMSE (mins)  RÂ² Score\n",
      "    Random Forest   17.040492    22.144296  0.819417\n",
      "Gradient Boosting   17.381916    22.353542  0.815988\n",
      "Linear Regression   26.396671    33.476125  0.587311\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model Comparison Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'MAE_mins': [results[m]['MAE'] for m in results.keys()],\n",
    "    'RMSE_mins': [results[m]['RMSE'] for m in results.keys()],\n",
    "    'R2_Score': [results[m]['RÂ²'] for m in results.keys()]\n",
    "})\n",
    "comparison_df = comparison_df.sort_values('MAE_mins')\n",
    "\n",
    "# Rename for display\n",
    "comparison_df.columns = ['Model', 'MAE (mins)', 'RMSE (mins)', 'RÂ² Score']\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2575957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† Best Model: Random Forest (Lowest MAE)\n",
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS (First 10 Test Cases)\n",
      "============================================================\n",
      "\n",
      "    Actual (mins)  LR Pred  RF Pred  GB Pred  RF Error  GB Error\n",
      "0            170   174.99   166.51   173.03      3.49      3.03\n",
      "1            180   148.95   187.53   188.46      7.53      8.46\n",
      "2            100   132.64   129.76   128.88     29.76     28.88\n",
      "3            170   157.05   172.58   173.34      2.58      3.34\n",
      "4             75   112.91    76.61    76.54      1.61      1.54\n",
      "5            115   110.24   108.31   101.31      6.69     13.69\n",
      "6             80   123.77   103.36   102.18     23.36     22.18\n",
      "7             50    53.27    65.75    68.16     15.75     18.16\n",
      "8             80    94.66    98.75    93.78     18.75     13.78\n",
      "9            100    97.06   121.82   112.03     21.82     12.03\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} (Lowest MAE)\")\n",
    "\n",
    "# Sample predictions comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS (First 10 Test Cases)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_df = pd.DataFrame({\n",
    "    'Actual (mins)': y_test[:10],\n",
    "    'LR Pred': lr_pred[:10],\n",
    "    'RF Pred': rf_pred[:10],\n",
    "    'GB Pred': gb_pred[:10]\n",
    "})\n",
    "\n",
    "# Add error columns\n",
    "sample_df['RF Error'] = np.abs(sample_df['Actual (mins)'] - sample_df['RF Pred'])\n",
    "sample_df['GB Error'] = np.abs(sample_df['Actual (mins)'] - sample_df['GB Pred'])\n",
    "\n",
    "print(\"\\n\", sample_df.round(2).to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07753765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MODELS\n",
      "============================================================\n",
      "âœ… Models saved successfully:\n",
      "   - lr_model.pkl\n",
      "   - rf_model.pkl\n",
      "   - gb_model.pkl\n",
      "   - best_model.pkl (Random Forest)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Save all models\n",
    "    with open('lr_model.pkl', 'wb') as f:\n",
    "        pickle.dump(lr_model, f)\n",
    "    with open('rf_model.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_model, f)\n",
    "    with open('gb_model.pkl', 'wb') as f:\n",
    "        pickle.dump(gb_model, f)\n",
    "    \n",
    "    print(\"âœ… Models saved successfully:\")\n",
    "    print(\"   - lr_model.pkl\")\n",
    "    print(\"   - rf_model.pkl\")\n",
    "    print(\"   - gb_model.pkl\")\n",
    "    \n",
    "    # Save best model separately\n",
    "    if best_model_name == 'Random Forest':\n",
    "        best_model = rf_model\n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        best_model = gb_model\n",
    "    else:\n",
    "        best_model = lr_model\n",
    "    \n",
    "    with open('best_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"   - best_model.pkl ({best_model_name})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26800b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP 10 FEATURE IMPORTANCES (Random Forest)\n",
      "============================================================\n",
      "\n",
      "            Feature  Importance\n",
      "  Category_Grocery    0.263181\n",
      "      Agent_Rating    0.178205\n",
      "       Traffic_Low    0.108092\n",
      "       distance_km    0.106506\n",
      "         Agent_Age    0.088069\n",
      "     Weather_Sunny    0.061605\n",
      "    Weather_Cloudy    0.045101\n",
      "       Weather_Fog    0.044034\n",
      "Vehicle_motorcycle    0.025630\n",
      "    Traffic_Medium    0.011865\n",
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 10 FEATURE IMPORTANCES (Random Forest)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Load preprocessor to get feature names\n",
    "        with open('preprocessor.pkl', 'rb') as f:\n",
    "            preprocessor = pickle.load(f)\n",
    "        \n",
    "        num_feats = ['distance_km', 'Agent_Age', 'Agent_Rating', 'pickup_delay_mins', 'order_hour', 'order_dayofweek']\n",
    "        cat_feats = ['Weather', 'Traffic', 'Vehicle', 'Area', 'Category']\n",
    "        \n",
    "        feature_names = (\n",
    "            num_feats + \n",
    "            preprocessor.named_transformers_['cat'].get_feature_names_out(cat_feats).tolist()\n",
    "        )\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': rf_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False).head(10)\n",
    "        \n",
    "        print(\"\\n\", importance_df.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not display feature importances: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47ab2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a104ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Train shape: (34918, 39)\n",
      "Test shape: (8730, 39)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8303064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RANDOM FOREST HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "\n",
      "Parameter grid:\n",
      "  n_estimators: [100, 200, 300]\n",
      "  max_depth: [10, 15, 20, None]\n",
      "  min_samples_split: [2, 5, 10]\n",
      "  min_samples_leaf: [1, 2, 4]\n",
      "  max_features: ['sqrt', 'log2']\n",
      "\n",
      "Running RandomizedSearchCV (20 iterations)...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "Tuning completed in 28.51 seconds\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Best CV MAE: 17.86 mins\n",
      "\n",
      "Test Set Performance:\n",
      "  MAE:  17.71 mins\n",
      "  RMSE: 22.72 mins\n",
      "  RÂ²:   0.8098\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST HYPERPARAMETER TUNING\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"\\nParameter grid:\")\n",
    "for key, value in rf_param_grid.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"\\nRunning RandomizedSearchCV (20 iterations)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=rf_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTuning completed in {rf_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best CV MAE: {-rf_random.best_score_:.2f} mins\")\n",
    "\n",
    "# Evaluate on test set\n",
    "rf_metrics = evaluate_model(rf_random.best_estimator_, X_test, y_test)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  MAE:  {rf_metrics['MAE']:.2f} mins\")\n",
    "print(f\"  RMSE: {rf_metrics['RMSE']:.2f} mins\")\n",
    "print(f\"  RÂ²:   {rf_metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "601b2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GRADIENT BOOSTING HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "\n",
      "Parameter grid:\n",
      "  n_estimators: [100, 200, 300]\n",
      "  learning_rate: [0.01, 0.05, 0.1, 0.2]\n",
      "  max_depth: [3, 5, 7]\n",
      "  min_samples_split: [2, 5, 10]\n",
      "  min_samples_leaf: [1, 2, 4]\n",
      "  subsample: [0.8, 0.9, 1.0]\n",
      "\n",
      "Running RandomizedSearchCV (20 iterations)...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "Tuning completed in 173.44 seconds\n",
      "Best parameters: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 7, 'learning_rate': 0.1}\n",
      "Best CV MAE: 17.30 mins\n",
      "\n",
      "Test Set Performance:\n",
      "  MAE:  17.19 mins\n",
      "  RMSE: 22.16 mins\n",
      "  RÂ²:   0.8191\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT BOOSTING HYPERPARAMETER TUNING\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRADIENT BOOSTING HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\nParameter grid:\")\n",
    "for key, value in gb_param_grid.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "gb_base = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "print(\"\\nRunning RandomizedSearchCV (20 iterations)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gb_random = RandomizedSearchCV(\n",
    "    estimator=gb_base,\n",
    "    param_distributions=gb_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_random.fit(X_train, y_train)\n",
    "gb_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTuning completed in {gb_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {gb_random.best_params_}\")\n",
    "print(f\"Best CV MAE: {-gb_random.best_score_:.2f} mins\")\n",
    "\n",
    "# Evaluate on test set\n",
    "gb_metrics = evaluate_model(gb_random.best_estimator_, X_test, y_test)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  MAE:  {gb_metrics['MAE']:.2f} mins\")\n",
    "print(f\"  RMSE: {gb_metrics['RMSE']:.2f} mins\")\n",
    "print(f\"  RÂ²:   {gb_metrics['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61131913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TUNED MODELS COMPARISON\n",
      "============================================================\n",
      "\n",
      "                     Model  MAE (mins)  RMSE (mins)  RÂ² Score  Training Time (s)\n",
      "    Random Forest (Tuned)   17.712655    22.724018  0.809838          28.510487\n",
      "Gradient Boosting (Tuned)   17.191519    22.161749  0.819132         173.444541\n",
      "\n",
      "ðŸ† Best Model: Gradient Boosting (Tuned)\n"
     ]
    }
   ],
   "source": [
    "# MODEL COMPARISON\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TUNED MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest (Tuned)', 'Gradient Boosting (Tuned)'],\n",
    "    'MAE (mins)': [rf_metrics['MAE'], gb_metrics['MAE']],\n",
    "    'RMSE (mins)': [rf_metrics['RMSE'], gb_metrics['RMSE']],\n",
    "    'RÂ² Score': [rf_metrics['R2'], gb_metrics['R2']],\n",
    "    'Training Time (s)': [rf_time, gb_time]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_idx = comparison['MAE (mins)'].idxmin()\n",
    "best_model_name = comparison.loc[best_idx, 'Model']\n",
    "best_model = rf_random.best_estimator_ if best_idx == 0 else gb_random.best_estimator_\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99c14fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING TUNED MODELS\n",
      "============================================================\n",
      "âœ… Models saved successfully:\n",
      "   - rf_tuned_model.pkl\n",
      "   - gb_tuned_model.pkl\n",
      "   - final_model.pkl\n",
      "   - model_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# SAVE TUNED MODELS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING TUNED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Save both tuned models\n",
    "    with open('rf_tuned_model.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_random.best_estimator_, f)\n",
    "    \n",
    "    with open('gb_tuned_model.pkl', 'wb') as f:\n",
    "        pickle.dump(gb_random.best_estimator_, f)\n",
    "    \n",
    "    # Save best model\n",
    "    with open('final_model.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'best_model': best_model_name,\n",
    "        'rf_params': rf_random.best_params_,\n",
    "        'gb_params': gb_random.best_params_,\n",
    "        'rf_metrics': rf_metrics,\n",
    "        'gb_metrics': gb_metrics,\n",
    "        'rf_training_time': rf_time,\n",
    "        'gb_training_time': gb_time\n",
    "    }\n",
    "    \n",
    "    with open('model_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(\"âœ… Models saved successfully:\")\n",
    "    print(\"   - rf_tuned_model.pkl\")\n",
    "    print(\"   - gb_tuned_model.pkl\")\n",
    "    print(\"   - final_model.pkl\")\n",
    "    print(\"   - model_metadata.pkl\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ecc32ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP 15 FEATURE IMPORTANCES (Best Model)\n",
      "============================================================\n",
      "\n",
      "            Feature  Importance\n",
      "  Category_Grocery    0.278530\n",
      "      Agent_Rating    0.180999\n",
      "       distance_km    0.108847\n",
      "       Traffic_Low    0.105791\n",
      "         Agent_Age    0.084338\n",
      "     Weather_Sunny    0.061871\n",
      "    Weather_Cloudy    0.046590\n",
      "       Weather_Fog    0.038858\n",
      "Vehicle_motorcycle    0.025819\n",
      "       Traffic_Jam    0.024220\n",
      "    Traffic_Medium    0.013744\n",
      "Area_Metropolitian    0.006433\n",
      "   Area_Semi-Urban    0.005179\n",
      "        order_hour    0.002508\n",
      "Weather_Sandstorms    0.002289\n",
      "\n",
      "âœ… Feature importance saved to 'feature_importance.csv'\n",
      "\n",
      "============================================================\n",
      "âœ… HYPERPARAMETER TUNING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# FEATURE IMPORTANCE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 15 FEATURE IMPORTANCES (Best Model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    with open('preprocessor.pkl', 'rb') as f:\n",
    "        preprocessor = pickle.load(f)\n",
    "    \n",
    "    num_feats = ['distance_km', 'Agent_Age', 'Agent_Rating', 'pickup_delay_mins', 'order_hour', 'order_dayofweek']\n",
    "    cat_feats = ['Weather', 'Traffic', 'Vehicle', 'Area', 'Category']\n",
    "    \n",
    "    feature_names = (\n",
    "        num_feats + \n",
    "        preprocessor.named_transformers_['cat'].get_feature_names_out(cat_feats).tolist()\n",
    "    )\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(15)\n",
    "    \n",
    "    print(\"\\n\", importance_df.to_string(index=False))\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    print(\"\\nâœ… Feature importance saved to 'feature_importance.csv'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not display feature importances: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
